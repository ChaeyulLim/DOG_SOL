
<artifact identifier="storage-structure-doc" type="text/markdown" title="12_STORAGE_êµ¬ì¡°ì„¤ëª….md">
# 12_STORAGE ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ëª…

> **ëª©ì **: ë°ì´í„° ì €ì¥ì†Œ êµ¬ì¡° ë° ê´€ë¦¬ ë°©ë²•

---

## ğŸ“‹ ëª©ì°¨
1. [ë””ë ‰í† ë¦¬ êµ¬ì¡°](#ë””ë ‰í† ë¦¬-êµ¬ì¡°)
2. [historical/ - ê³¼ê±° ë°ì´í„°](#historical---ê³¼ê±°-ë°ì´í„°)
3. [cache/ - ì„ì‹œ ìºì‹œ](#cache---ì„ì‹œ-ìºì‹œ)
4. [trades.db - ê±°ë˜ ë°ì´í„°ë² ì´ìŠ¤](#tradesdb---ê±°ë˜-ë°ì´í„°ë² ì´ìŠ¤)
5. [ê´€ë¦¬ ë° ìœ ì§€ë³´ìˆ˜](#ê´€ë¦¬-ë°-ìœ ì§€ë³´ìˆ˜)

---

## ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
storage/
â”œâ”€â”€ historical/              # ë°±í…ŒìŠ¤íŠ¸ìš© ê³¼ê±° ë°ì´í„°
â”‚   â”œâ”€â”€ DOGE_USDT_5m.csv
â”‚   â”œâ”€â”€ SOL_USDT_5m.csv
â”‚   â”œâ”€â”€ DOGE_USDT_1h.csv
â”‚   â”œâ”€â”€ SOL_USDT_1h.csv
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ cache/                   # 1ë¶„ê°„ ì„ì‹œ ìºì‹œ
â”‚   â”œâ”€â”€ market_data.json
â”‚   â”œâ”€â”€ indicators.json
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ system_state.json        # ì‹œìŠ¤í…œ ìƒíƒœ ë°±ì—… (5ë¶„ë§ˆë‹¤)
â”œâ”€â”€ trades.db               # SQLite ê±°ë˜ DB
â”œâ”€â”€ trades.db-journal       # SQLite ì €ë„ (ìë™ ìƒì„±)
â””â”€â”€ README.md
```

---

## historical/ - ê³¼ê±° ë°ì´í„°

### ëª©ì 
ë°±í…ŒìŠ¤íŠ¸ìš© ê³¼ê±° OHLCV ë°ì´í„° ì €ì¥

### CSV íŒŒì¼ í˜•ì‹

**íŒŒì¼ëª… ê·œì¹™**:
```
{SYMBOL}_{TIMEFRAME}.csv
ì˜ˆ: DOGE_USDT_5m.csv, SOL_USDT_1h.csv
```

**CSV êµ¬ì¡°**:
```csv
timestamp,open,high,low,close,volume
1640000000,0.3821,0.3895,0.3800,0.3850,1234567890
1640000300,0.3850,0.3920,0.3840,0.3900,1345678901
...
```

**ì»¬ëŸ¼ ì„¤ëª…**:
- `timestamp`: Unix íƒ€ì„ìŠ¤íƒ¬í”„ (ì´ˆ)
- `open`: ì‹œê°€ (USDT)
- `high`: ê³ ê°€ (USDT)
- `low`: ì €ê°€ (USDT)
- `close`: ì¢…ê°€ (USDT)
- `volume`: ê±°ë˜ëŸ‰ (ì½”ì¸ ìˆ˜ëŸ‰)

### ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•

**ì˜µì…˜ 1: Bybit APIë¡œ ì§ì ‘ ìˆ˜ì§‘**
```python
# scripts/download_historical_data.py
import ccxt
import pandas as pd
from datetime import datetime, timedelta

def download_historical_data(symbol, timeframe, days):
    exchange = ccxt.bybit()
    
    since = exchange.parse8601(
        (datetime.now() - timedelta(days=days)).isoformat()
    )
    
    all_ohlcv = []
    
    while True:
        ohlcv = exchange.fetch_ohlcv(
            symbol, 
            timeframe, 
            since=since,
            limit=1000
        )
        
        if not ohlcv:
            break
        
        all_ohlcv.extend(ohlcv)
        since = ohlcv[-1][0] + 1
        
        if len(ohlcv) < 1000:
            break
    
    # CSV ì €ì¥
    df = pd.DataFrame(
        all_ohlcv,
        columns=['timestamp', 'open', 'high', 'low', 'close', 'volume']
    )
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
    df['timestamp'] = df['timestamp'] // 1000
    
    filename = f"storage/historical/{symbol.replace('/', '_')}_{timeframe}.csv"
    df.to_csv(filename, index=False)
    
    print(f"âœ… {filename} ì €ì¥ ì™„ë£Œ ({len(df)}ê°œ)")

# ì‚¬ìš©
download_historical_data('DOGE/USDT', '5m', 180)  # 6ê°œì›”
download_historical_data('SOL/USDT', '5m', 180)
```

**ì˜µì…˜ 2: ì™¸ë¶€ ë°ì´í„° ì‚¬ìš©**
- CryptoDataDownload: https://www.cryptodatadownload.com/
- Binance Vision: https://data.binance.vision/
- í˜•ì‹ ë§ì¶°ì„œ ì €ì¥

### ë°ì´í„° ê²€ì¦
```python
# scripts/validate_historical_data.py
import pandas as pd

def validate_csv(filepath):
    df = pd.read_csv(filepath)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
    assert all(col in df.columns for col in required)
    
    # ë°ì´í„° íƒ€ì… í™•ì¸
    assert df['timestamp'].dtype in ['int64', 'int32']
    assert all(df[col].dtype == 'float64' for col in ['open', 'high', 'low', 'close', 'volume'])
    
    # ë…¼ë¦¬ ê²€ì¦
    assert (df['high'] >= df['low']).all()
    assert (df['high'] >= df['open']).all()
    assert (df['high'] >= df['close']).all()
    assert (df['low'] <= df['open']).all()
    assert (df['low'] <= df['close']).all()
    
    # ì‹œê°„ ìˆœì„œ í™•ì¸
    assert df['timestamp'].is_monotonic_increasing
    
    print(f"âœ… {filepath} ê²€ì¦ ì™„ë£Œ")
    print(f"   ë°ì´í„° ê°œìˆ˜: {len(df)}")
    print(f"   ê¸°ê°„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
    
validate_csv('storage/historical/DOGE_USDT_5m.csv')
```

### README.md (historical/)
```markdown
# Historical Data

ë°±í…ŒìŠ¤íŠ¸ìš© ê³¼ê±° OHLCV ë°ì´í„°

## íŒŒì¼ ëª©ë¡
- `DOGE_USDT_5m.csv`: DOGE 5ë¶„ë´‰ (6ê°œì›”)
- `SOL_USDT_5m.csv`: SOL 5ë¶„ë´‰ (6ê°œì›”)

## ë°ì´í„° ê°±ì‹ 
```bash
python scripts/download_historical_data.py
```

## ì£¼ì˜ì‚¬í•­
- ìš©ëŸ‰ì´ í¬ë¯€ë¡œ gitì— ì»¤ë°‹í•˜ì§€ ì•ŠìŒ (.gitignore í¬í•¨)
- ìµœì†Œ 6ê°œì›” ì´ìƒ ë°ì´í„° ê¶Œì¥
```

---

## cache/ - ì„ì‹œ ìºì‹œ

### ëª©ì 
1ë¶„ê°„ ìœ íš¨í•œ API ì‘ë‹µ ìºì‹œ

### ìºì‹œ íŒŒì¼ êµ¬ì¡°

**market_data.json**:
```json
{
  "DOGE/USDT": {
    "price": 0.3821,
    "volume_24h": 1234567890,
    "timestamp": 1640000000,
    "expires_at": 1640000060
  },
  "SOL/USDT": {
    "price": 98.52,
    "volume_24h": 9876543210,
    "timestamp": 1640000000,
    "expires_at": 1640000060
  }
}
```

**indicators.json**:
```json
{
  "DOGE/USDT": {
    "rsi": {"value": 45.2, "oversold": false},
    "macd": {"value": 0.0015, "golden_cross": true},
    "timestamp": 1640000000,
    "expires_at": 1640000060
  }
}
```

### ìºì‹œ ê´€ë¦¬

**data/cache.pyì—ì„œ ìë™ ê´€ë¦¬**:
```python
# ì½ê¸°
cached = cache.get('market_data', 'DOGE/USDT')
if cached and cached['expires_at'] > time.time():
    return cached

# ì“°ê¸°
cache.set('market_data', 'DOGE/USDT', {
    'price': 0.3821,
    'timestamp': time.time(),
    'expires_at': time.time() + 60  # 1ë¶„ í›„ ë§Œë£Œ
})
```

### .gitkeep
```
# ë¹ˆ ë””ë ‰í† ë¦¬ë¥¼ gitì— í¬í•¨ì‹œí‚¤ê¸° ìœ„í•œ íŒŒì¼
```

---

## trades.db - ê±°ë˜ ë°ì´í„°ë² ì´ìŠ¤

### ëª©ì 
ëª¨ë“  ê±°ë˜ ê¸°ë¡ ì˜êµ¬ ì €ì¥ (SQLite)

### ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

**trades í…Œì´ë¸”**:
```sql
CREATE TABLE trades (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp DATETIME NOT NULL,
    symbol TEXT NOT NULL,
    mode TEXT NOT NULL,  -- 'paper', 'live', 'backtest'
    
    -- ê±°ë˜ ì •ë³´
    entry_price REAL NOT NULL,
    exit_price REAL,
    quantity REAL NOT NULL,
    pnl_percent REAL,
    pnl_krw REAL,
    
    -- ì§€í‘œ
    rsi_entry REAL,
    macd_entry TEXT,  -- JSON
    bb_entry TEXT,    -- JSON
    volume_ratio REAL,
    
    -- AI
    ai_confidence REAL,
    ai_reasoning TEXT,
    
    -- ì²­ì‚°
    exit_reason TEXT,
    exit_timestamp DATETIME,
    holding_minutes INTEGER,
    
    -- ìˆ˜ìˆ˜ë£Œ
    entry_fee REAL,
    exit_fee REAL,
    
    -- ìƒíƒœ
    status TEXT DEFAULT 'OPEN',  -- 'OPEN', 'CLOSED'
    
    INDEX idx_timestamp (timestamp),
    INDEX idx_symbol (symbol),
    INDEX idx_status (status)
);
```

**learning_data í…Œì´ë¸”**:
```sql
CREATE TABLE learning_data (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    trade_id INTEGER NOT NULL,
    timestamp DATETIME NOT NULL,
    
    entry_features TEXT NOT NULL,  -- JSON
    exit_features TEXT,            -- JSON
    market_conditions TEXT,        -- JSON
    outcome TEXT NOT NULL,          -- JSON
    patterns_detected TEXT,        -- JSON
    
    FOREIGN KEY (trade_id) REFERENCES trades(id)
);
```

**risk_events í…Œì´ë¸”**:
```sql
CREATE TABLE risk_events (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp DATETIME NOT NULL,
    event_type TEXT NOT NULL,
    details TEXT,      -- JSON
    action_taken TEXT
);
```

### ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬

**ì´ˆê¸° ìƒì„±** (database/models.py):
```python
import sqlite3

def initialize_database():
    conn = sqlite3.connect('storage/trades.db')
    cursor = conn.cursor()
    
    # trades í…Œì´ë¸” ìƒì„±
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS trades (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp DATETIME NOT NULL,
            ...
        )
    ''')
    
    # ì¸ë±ìŠ¤ ìƒì„±
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON trades(timestamp)')
    
    conn.commit()
    conn.close()
```

**ë°±ì—…**:
```bash
# ë§¤ì¼ ìë™ ë°±ì—…
cp storage/trades.db storage/backups/trades_$(date +%Y%m%d).db
```

**ìš©ëŸ‰ ê´€ë¦¬**:
```python
# ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ (ì„ íƒ)
DELETE FROM trades WHERE timestamp < date('now', '-1 year');
VACUUM;  # DB íŒŒì¼ í¬ê¸° ìµœì í™”
```

---

## system_state.json

### ëª©ì 
ì‹œìŠ¤í…œ ë¹„ì •ìƒ ì¢…ë£Œ ì‹œ ë³µêµ¬ìš© ìƒíƒœ ì €ì¥

### ì €ì¥ ë‚´ìš©
```json
{
  "timestamp": 1640000000,
  "mode": "paper",
  "balance": 1035420,
  "positions": {
    "DOGE/USDT": {
      "trade_id": 123,
      "entry_price": 0.3821,
      "quantity": 1006,
      "entry_time": 1640000000
    }
  },
  "risk_state": {
    "daily_loss": 0.012,
    "monthly_dd": -0.021,
    "consecutive_losses": 0
  },
  "config": {
    "INVESTMENT_AMOUNT": 1000000,
    "TAKE_PROFIT": 0.02,
    "STOP_LOSS": -0.01
  }
}
```

### ìë™ ì €ì¥
```python
# engine/base_engine.py
async def save_state_periodically(self):
    """5ë¶„ë§ˆë‹¤ ìë™ ì €ì¥"""
    while self.running:
        state = {
            'timestamp': time.time(),
            'balance': self.get_current_balance(),
            'positions': self.get_open_positions(),
            'risk_state': self.risk_manager.get_state(),
            'config': self.config.to_dict()
        }
        
        with open('storage/system_state.json', 'w') as f:
            json.dump(state, f, indent=2)
        
        await asyncio.sleep(300)  # 5ë¶„
```

### ë³µêµ¬ ì‚¬ìš©
```python
# engine/base_engine.py
async def recover_from_crash(self):
    """ì¬ì‹œì‘ ì‹œ ìƒíƒœ ë³µêµ¬"""
    try:
        with open('storage/system_state.json', 'r') as f:
            state = json.load(f)
        
        logger.info("=== ì‹œìŠ¤í…œ ìƒíƒœ ë³µêµ¬ ì‹œì‘ ===")
        
        # í¬ì§€ì…˜ ë³µêµ¬
        for symbol, pos in state['positions'].items():
            self.restore_position(symbol, pos)
        
        # ë¦¬ìŠ¤í¬ ìƒíƒœ ë³µêµ¬
        self.risk_manager.restore_state(state['risk_state'])
        
        logger.info("=== ì‹œìŠ¤í…œ ìƒíƒœ ë³µêµ¬ ì™„ë£Œ ===")
    
    except FileNotFoundError:
        logger.info("ë³µêµ¬í•  ìƒíƒœ ì—†ìŒ, ì •ìƒ ì‹œì‘")
```

---

## ê´€ë¦¬ ë° ìœ ì§€ë³´ìˆ˜

### ë””ìŠ¤í¬ ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```python
import shutil

def check_disk_space():
    """ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„ í™•ì¸"""
    total, used, free = shutil.disk_usage('/storage')
    
    free_gb = free // (2**30)
    
    if free_gb < 1:  # 1GB ë¯¸ë§Œ
        logger.warning(f"âš ï¸ ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„ ë¶€ì¡±: {free_gb}GB")
        return False
    
    return True
```

### ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸

**scripts/cleanup_storage.py**:
```python
import os
from datetime import datetime, timedelta

def cleanup_old_cache():
    """ì˜¤ë˜ëœ ìºì‹œ ì‚­ì œ"""
    cache_dir = 'storage/cache'
    
    for filename in os.listdir(cache_dir):
        if filename == '.gitkeep':
            continue
        
        filepath = os.path.join(cache_dir, filename)
        
        # 1ì‹œê°„ ì´ìƒ ëœ íŒŒì¼ ì‚­ì œ
        if os.path.getmtime(filepath) < time.time() - 3600:
            os.remove(filepath)
            print(f"ì‚­ì œ: {filename}")

def backup_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…"""
    import shutil
    from datetime import datetime
    
    src = 'storage/trades.db'
    
    # ë°±ì—… ë””ë ‰í† ë¦¬
    backup_dir = 'storage/backups'
    os.makedirs(backup_dir, exist_ok=True)
    
    # ë‚ ì§œë³„ ë°±ì—…
    date_str = datetime.now().strftime('%Y%m%d')
    dst = f'{backup_dir}/trades_{date_str}.db'
    
    shutil.copy2(src, dst)
    print(f"âœ… DB ë°±ì—… ì™„ë£Œ: {dst}")

def remove_old_backups(days=30):
    """ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ"""
    backup_dir = 'storage/backups'
    cutoff = time.time() - (days * 86400)
    
    for filename in os.listdir(backup_dir):
        filepath = os.path.join(backup_dir, filename)
        
        if os.path.getmtime(filepath) < cutoff:
            os.remove(filepath)
            print(f"ì‚­ì œ: {filename}")

if __name__ == '__main__':
    cleanup_old_cache()
    backup_database()
    remove_old_backups(30)
```

### ìë™í™” (cron)

**Linux/Mac**:
```bash
# crontab -e
# ë§¤ì¼ ìƒˆë²½ 3ì‹œ ë°±ì—… ë° ì •ë¦¬
0 3 * * * cd /path/to/project && python scripts/cleanup_storage.py
```

**Windows (Task Scheduler)**:
```
ì‘ì—…: cleanup_storage
íŠ¸ë¦¬ê±°: ë§¤ì¼ ì˜¤ì „ 3:00
ë™ì‘: python scripts/cleanup_storage.py
```

---

## ë””ë ‰í† ë¦¬ë³„ ìš©ëŸ‰ ê°€ì´ë“œ

```
storage/
â”œâ”€â”€ historical/    ~500MB-2GB  (6ê°œì›” 5ë¶„ë´‰ ê¸°ì¤€)
â”œâ”€â”€ cache/         ~1-10MB     (í•­ìƒ ì‘ìŒ)
â”œâ”€â”€ trades.db      ~10-100MB   (1ë…„ ê±°ë˜ ê¸°ì¤€)
â”œâ”€â”€ backups/       ~300MB-3GB  (30ì¼ ë°±ì—…)
â””â”€â”€ system_state   ~1-10KB     (ë¬´ì‹œ ê°€ëŠ¥)

ì´ ì˜ˆìƒ ìš©ëŸ‰: 1-5GB
```

### ìš©ëŸ‰ ì ˆì•½ íŒ

1. **Historical ë°ì´í„°**:
   - í•„ìš”í•œ ê¸°ê°„ë§Œ ìœ ì§€ (6-12ê°œì›”)
   - ì••ì¶• ì €ì¥ (gzip)
   ```bash
   gzip storage/historical/*.csv
   ```

2. **ë°ì´í„°ë² ì´ìŠ¤**:
   - ì •ê¸°ì  VACUUM
   ```sql
   VACUUM;
   ```
   - ì˜¤ë˜ëœ ë°ì´í„° ì•„ì¹´ì´ë¸Œ

3. **ë°±ì—…**:
   - ì••ì¶• ë°±ì—…
   ```bash
   tar -czf trades_backup.tar.gz storage/trades.db
   ```

---

## .gitignore ì„¤ì •

```gitignore
# Storage ë””ë ‰í† ë¦¬
storage/historical/*.csv
storage/cache/*
!storage/cache/.gitkeep
storage/trades.db
storage/trades.db-journal
storage/system_state.json
storage/backups/

# ë‹¨, êµ¬ì¡°ëŠ” ìœ ì§€
!storage/README.md
!storage/historical/README.md
```

---

## ì´ˆê¸° ì„¤ì • ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê°œë°œ í™˜ê²½
- [ ] `storage/` ë””ë ‰í† ë¦¬ ìƒì„±
- [ ] `storage/historical/` ìƒì„±
- [ ] `storage/cache/` ìƒì„± + `.gitkeep`
- [ ] `storage/backups/` ìƒì„±
- [ ] ê³¼ê±° ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ë°±í…ŒìŠ¤íŠ¸ìš©)
- [ ] DB ì´ˆê¸°í™” (`initialize_database()`)

### í”„ë¡œë•ì…˜ í™˜ê²½
- [ ] ë””ìŠ¤í¬ ìš©ëŸ‰ í™•ì¸ (ìµœì†Œ 5GB)
- [ ] ë°±ì—… ìë™í™” ì„¤ì • (cron/Task Scheduler)
- [ ] ëª¨ë‹ˆí„°ë§ ì„¤ì • (ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰)
- [ ] ê¶Œí•œ ì„¤ì • (`chmod 755 storage/`)

---

## ë¬¸ì œ í•´ê²°

### Q: trades.dbê°€ ì ê²¨ìˆì–´ìš”
**A**: SQLite ì €ë„ íŒŒì¼ ì‚­ì œ
```bash
rm storage/trades.db-journal
```

### Q: ìºì‹œê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”
**A**: cache/ ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸
```bash
chmod 755 storage/cache/
```

### Q: ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ì–´ìš”
**A**: ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
```bash
python scripts/download_historical_data.py
```

### Q: ë””ìŠ¤í¬ ìš©ëŸ‰ ë¶€ì¡±
**A**: ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
```bash
python scripts/cleanup_storage.py
```

---

## README.md (storage/)

```markdown
# Storage Directory

ë°ì´í„° ì €ì¥ì†Œ

## êµ¬ì¡°
```
storage/
â”œâ”€â”€ historical/     # ë°±í…ŒìŠ¤íŠ¸ìš© ê³¼ê±° ë°ì´í„°
â”œâ”€â”€ cache/          # 1ë¶„ ìºì‹œ
â”œâ”€â”€ trades.db       # ê±°ë˜ DB
â””â”€â”€ system_state    # ì‹œìŠ¤í…œ ìƒíƒœ ë°±ì—…
```

## ì´ˆê¸° ì„¤ì •
```bash
# ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p storage/historical storage/cache storage/backups

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
python -c "from database.models import initialize_database; initialize_database()"

# ê³¼ê±° ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì„ íƒ)
python scripts/download_historical_data.py
```

## ìœ ì§€ë³´ìˆ˜
```bash
# ë°±ì—…
python scripts/cleanup_storage.py

# ìš©ëŸ‰ í™•ì¸
du -sh storage/*
```

## ì£¼ì˜ì‚¬í•­
- `trades.db`ëŠ” ì¤‘ìš” ë°ì´í„°ì´ë¯€ë¡œ ì •ê¸° ë°±ì—… í•„ìˆ˜
- `historical/` CSVëŠ” ìš©ëŸ‰ì´ í¬ë¯€ë¡œ gitì—ì„œ ì œì™¸
- `cache/`ëŠ” ìë™ìœ¼ë¡œ ê´€ë¦¬ë˜ë¯€ë¡œ ìˆ˜ë™ ì‚­ì œ ë¶ˆí•„ìš”
```

---

**ë¬¸ì„œ ë²„ì „**: v1.0  
**ì‘ì„±ì¼**: 2025-01-15  
**Phase**: 12 (ë°ì´í„° ì €ì¥ì†Œ)  
**ê²€ì¦**: âœ… ì™„ë£Œ
</artifact>

<artifact identifier="logs-structure-doc" type="text/markdown" title="13_LOGS_êµ¬ì¡°ì„¤ëª….md">
# 13_LOGS ë””ë ‰í† ë¦¬ êµ¬ì¡° ì„¤ëª…

> **ëª©ì **: ë¡œê·¸ íŒŒì¼ êµ¬ì¡° ë° ê´€ë¦¬ ë°©ë²•

---

## ğŸ“‹ ëª©ì°¨
1. [ë””ë ‰í† ë¦¬ êµ¬ì¡°](#ë””ë ‰í† ë¦¬-êµ¬ì¡°)
2. [ë¡œê·¸ ë ˆë²¨ ë° ë¶„ë¥˜](#ë¡œê·¸-ë ˆë²¨-ë°-ë¶„ë¥˜)
3. [ëª¨ë“œë³„ ë¡œê·¸](#ëª¨ë“œë³„-ë¡œê·¸)
4. [ë¡œê·¸ í¬ë§·](#ë¡œê·¸-í¬ë§·)
5. [ë¡œê·¸ ë¡œí…Œì´ì…˜](#ë¡œê·¸-ë¡œí…Œì´ì…˜)
6. [ê´€ë¦¬ ë° ë¶„ì„](#ê´€ë¦¬-ë°-ë¶„ì„)

---

## ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
logs/
â”œâ”€â”€ paper/                      # ëª¨ì˜íˆ¬ì ë¡œê·¸
â”‚   â”œâ”€â”€ 2025-01-15_trades.log      # ê±°ë˜ë§Œ
â”‚   â”œâ”€â”€ 2025-01-15_decisions.log   # AI íŒë‹¨ë§Œ
â”‚   â”œâ”€â”€ 2025-01-15_errors.log      # ì—ëŸ¬ë§Œ
â”‚   â”œâ”€â”€ 2025-01-15_debug.log       # ì „ì²´ (DEBUG ëª¨ë“œ)
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ live/                       # ì‹¤ê±°ë˜ ë¡œê·¸
â”‚   â”œâ”€â”€ 2025-01-15_trades.log
â”‚   â”œâ”€â”€ 2025-01-15_decisions.log
â”‚   â”œâ”€â”€ 2025-01-15_errors.log
â”‚   â”œâ”€â”€ 2025-01-15_debug.log
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ backtest/                   # ë°±í…ŒìŠ¤íŠ¸ ë¡œê·¸
â”‚   â”œâ”€â”€ 2025-01-15_backtest.log
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ reports/                    # ë¦¬í¬íŠ¸ (monitoring/reporter.py)
â”‚   â”œâ”€â”€ daily/
â”‚   â”‚   â”œâ”€â”€ 2025-01-15.txt
â”‚   â”‚   â””â”€â”€ 2025-01-16.txt
â”‚   â”œâ”€â”€ weekly/
â”‚   â”‚   â”œâ”€â”€ 2025-W03.txt
â”‚   â”‚   â””â”€â”€ 2025-W04.txt
â”‚   â””â”€â”€ monthly/
â”‚       â””â”€â”€ 2025-01.txt
â”‚
â””â”€â”€ README.md
```

---

## ë¡œê·¸ ë ˆë²¨ ë° ë¶„ë¥˜

### Python Logging ë ˆë²¨
```python
DEBUG    10  # ìƒì„¸í•œ ë””ë²„ê¹… ì •ë³´
INFO     20  # ì¼ë°˜ ì •ë³´ (ê±°ë˜, íŒë‹¨)
WARNING  30  # ê²½ê³  (ë¦¬ìŠ¤í¬ ê·¼ì ‘)
ERROR    40  # ì˜¤ë¥˜ (ë³µêµ¬ ê°€ëŠ¥)
CRITICAL 50  # ì¹˜ëª…ì  ì˜¤ë¥˜ (ì‹œìŠ¤í…œ ì¤‘ë‹¨)
```

### ë¡œê·¸ íŒŒì¼ë³„ ë ˆë²¨

| íŒŒì¼ | ë ˆë²¨ | ë‚´ìš© | í•„í„° |
|------|------|------|------|
| `trades.log` | INFO | ê±°ë˜ ê¸°ë¡ | `[ENTRY]`, `[EXIT]` |
| `decisions.log` | INFO | AI íŒë‹¨ | `[AI]` |
| `errors.log` | ERROR+ | ì˜¤ë¥˜ | ERROR, CRITICAL |
| `debug.log` | DEBUG+ | ì „ì²´ | ëª¨ë“  ë ˆë²¨ |

---

## ëª¨ë“œë³„ ë¡œê·¸

### paper/ - ëª¨ì˜íˆ¬ì

**ìš©ë„**: ì „ëµ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

**ì£¼ìš” ë¡œê·¸**:
```
[2025-01-15 14:32:15] [INFO] [ENTRY] DOGE/USDT @ 0.3821 | Amount: 500,000 KRW | AI: 0.75
[2025-01-15 14:32:16] [INFO] [ORDER] Paper Order Filled: 1006 DOGE
[2025-01-15 16:45:22] [INFO] [EXIT] DOGE/USDT @ 0.3895 | PnL: +1.94% (+9,700 KRW) | Reason: TRAILING_STOP
[2025-01-15 18:00:00] [WARNING] [RISK] Daily Loss: -2.3% / -5.0%
```

**ë³´ê´€ ê¸°ê°„**: 30ì¼

---

### live/ - ì‹¤ê±°ë˜

**ìš©ë„**: ì‹¤ì œ ìê¸ˆ ê±°ë˜ ê¸°ë¡ (ì˜êµ¬ ë³´ê´€)

**ì£¼ìš” ë¡œê·¸**:
```
[2025-01-15 14:32:15] [INFO] [ENTRY] DOGE/USDT @ 0.3821 | Amount: 500,000 KRW | AI: 0.75
[2025-01-15 14:32:16] [INFO] [ORDER] Bybit Order ID: 1234567890 | Status: Filled
[2025-01-15 14:32:17] [INFO] [ORDER] Filled: 1006/1006 DOGE | Avg Price: 0.3821
[2025-01-15 16:45:22] [INFO] [EXIT] DOGE/USDT @ 0.3895 | PnL: +1.94% | Reason: TRAILING_STOP
[2025-01-15 16:45:23] [INFO] [ORDER] Bybit Order ID: 1234567891 | Status: Filled
[2025-01-15 18:00:00] [WARNING] [RISK] Daily Loss: -4.2% / -5.0% âš ï¸
```

**ë³´ê´€ ê¸°ê°„**: ì˜êµ¬ (ì„¸ê¸ˆ ì‹ ê³ ìš©)

---

### backtest/ - ë°±í…ŒìŠ¤íŠ¸

**ìš©ë„**: ê³¼ê±° ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜

**ë¡œê·¸ ì˜ˆì‹œ**:
```
[2025-01-15 10:00:00] [INFO] Backtest Started: 2024-01-01 ~ 2024-12-31
[2025-01-15 10:00:05] [INFO] [ENTRY] 2024-01-15 DOGE @ 0.3821
[2025-01-15 10:00:06] [INFO] [EXIT] 2024-01-15 DOGE @ 0.3895 | PnL: +1.94%
...
[2025-01-15 10:05:00] [INFO] Backtest Completed
[2025-01-15 10:05:01] [INFO] Total Trades: 127 | Win Rate: 65.4% | Sharpe: 1.85
```

**ë³´ê´€ ê¸°ê°„**: 90ì¼

---

## ë¡œê·¸ í¬ë§·

### ê¸°ë³¸ í¬ë§·
```
[YYYY-MM-DD HH:MM:SS] [LEVEL] [TAG] Message
```

### íƒœê·¸ë³„ í¬ë§·

**[ENTRY] - ì§„ì… ë¡œê·¸**:
```
[2025-01-15 14:32:15] [INFO] [ENTRY] {symbol} @ {price} | Amount: {amount} KRW | AI: {confidence}
```

**[EXIT] - ì²­ì‚° ë¡œê·¸**:
```
[2025-01-15 16:45:22] [INFO] [EXIT] {symbol} @ {exit_price} | Entry: {entry_price} | PnL: {pnl}% ({pnl_krw} KRW) | Reason: {reason}
```

**[AI] - AI íŒë‹¨ ë¡œê·¸**:
```
[2025-01-15 14:32:10] [INFO] [AI] {symbol} | Action: {action} | Confidence: {conf} | Reasoning: {reason}
```

**[RISK] - ë¦¬ìŠ¤í¬ ì´ë²¤íŠ¸**:
```
[2025-01-15 18:00:00] [WARNING] [RISK] {event_type} | Loss: {loss}% | Resume: {resume_time}
```

**[ORDER] - ì£¼ë¬¸ ìƒíƒœ**:
```
[2025-01-15 14:32:16] [INFO] [ORDER] Bybit Order ID: {order_id} | Status: {status} | Filled: {filled}/{total}
```

**[API] - API í˜¸ì¶œ**:
```
[2025-01-15 14:32:10] [DEBUG] [API] Bybit fetch_ticker DOGE/USDT | Response Time: 123ms
[2025-01-15 20:15:33] [ERROR] [API] Bybit Connection Timeout - Retrying (3/60)...
```

**[NETWORK] - ë„¤íŠ¸ì›Œí¬ ìƒíƒœ**:
```
[2025-01-15 14:23:00] [WARNING] [NETWORK] Connection Lost - Waiting for Recovery...
[2025-01-15 14:23:30] [INFO] [NETWORK] Connection Restored (30s downtime)
```

---

## ë¡œê·¸ ë¡œí…Œì´ì…˜

### ì¼ë³„ ë¡œí…Œì´ì…˜ (ê¸°ë³¸)

**êµ¬í˜„** (monitoring/logger.py):
```python
def setup_logger(self) -> logging.Logger:
    today = datetime.now().strftime('%Y-%m-%d')
    
    log_dir = Path(LOG_DIR) / self.mode
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # ê±°ë˜ ë¡œê·¸
    trades_handler = logging.FileHandler(
        log_dir / f'{today}_trades.log',
        encoding='utf-8'
    )
    # ...
```

### TimedRotatingFileHandler (ì„ íƒ)

**ìë™ ë¡œí…Œì´ì…˜**:
```python
from logging.handlers import TimedRotatingFileHandler

handler = TimedRotatingFileHandler(
    filename='logs/paper/trades.log',
    when='midnight',      # ìì •ë§ˆë‹¤
    interval=1,           # 1ì¼
    backupCount=30,       # 30ê°œ ë³´ê´€
    encoding='utf-8'
)
```

### ìˆ˜ë™ ì•„ì¹´ì´ë¸Œ

**scripts/archive_logs.py**:
```python
import os
import shutil
from datetime import datetime, timedelta

def archive_old_logs(mode='paper', days=30):
    """30ì¼ ì´ìƒ ëœ ë¡œê·¸ ì••ì¶•"""
    log_dir = f'logs/{mode}'
    archive_dir = f'logs/archives/{mode}'
    
    os.makedirs(archive_dir, exist_ok=True)
    
    cutoff_date = datetime.now() - timedelta(days=days)
    
    for filename in os.listdir(log_dir):
        if not filename.endswith('.log'):
            continue
        
        filepath = os.path.join(log_dir, filename)
        
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
        mtime = datetime.fromtimestamp(os.path.getmtime(filepath))
        
        if mtime < cutoff_date:
            # ì••ì¶• í›„ ì´ë™
            import gzip
            
            gz_filename = f'{filename}.gz'
            gz_filepath = os.path.join(archive_dir, gz_filename)
            
            with open(filepath, 'rb') as f_in:
                with gzip.open(gz_filepath, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            # ì›ë³¸ ì‚­ì œ
            os.remove(filepath)
            
            print(f"âœ… ì•„ì¹´ì´ë¸Œ: {filename} â†’ {gz_filename}")

# ì‚¬ìš©
archive_old_logs('paper', days=30)
archive_old_logs('live', days=365)  # LiveëŠ” 1ë…„ ë³´ê´€
```

---

## ê´€ë¦¬ ë° ë¶„ì„

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

**Linux/Mac**:
```bash
# ì „ì²´ ë¡œê·¸ ì‹¤ì‹œê°„ ë³´ê¸°
tail -f logs/paper/2025-01-15_debug.log

# ê±°ë˜ë§Œ ë³´ê¸°
tail -f logs/paper/2025-01-15_trades.log

# ì—ëŸ¬ë§Œ ë³´ê¸°
tail -f logs/paper/2025-01-15_errors.log

# ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ë³´ê¸°
tail -f logs/paper/2025-01-15_*.log
```

**Windows**:
```powershell
# PowerShell
Get-Content logs\paper\2025-01-15_debug.log -Wait -Tail 50
```

### ë¡œê·¸ ê²€ìƒ‰

**ì˜¤ëŠ˜ ì§„ì… ê±°ë˜ ì°¾ê¸°**:
```bash
grep "\[ENTRY\]" logs/paper/$(date +%Y-%m-%d)_trades.log
```

**ì˜¤ëŠ˜ ì†ì‹¤ ê±°ë˜ ì°¾ê¸°**:
```bash
grep "PnL: -" logs/paper/$(date +%Y-%m-%d)_trades.log
```

**íŠ¹ì • ì‹¬ë³¼ ê±°ë˜**:
```bash
grep "DOGE" logs/paper/$(date +%Y-%m-%d)_trades.log
```

**ì—ëŸ¬ ë°œìƒ íšŸìˆ˜**:
```bash
grep -c "\[ERROR\]" logs/paper/$(date +%Y-%m-%d)_errors.log
```

### ë¡œê·¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

**scripts/analyze_logs.py**:
```python
import re
from collections import defaultdict

def analyze_trades_log(filepath):
    """ê±°ë˜ ë¡œê·¸ ë¶„ì„"""
    trades = []
    
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            if '[EXIT]' in line:
                # PnL ì¶”ì¶œ
                match = re.search(r'PnL: ([+-]\d+\.\d+)%', line)
                if match:
                    pnl = float(match.group(1))
                    trades.append(pnl)
    
    if not trades:
        print("ê±°ë˜ ì—†ìŒ")
        return
    
    # í†µê³„
    total = len(trades)
    winners = [t for t in trades if t > 0]
    losers = [t for t in trades if t <= 0]
    
    win_rate = len(winners) / total * 100
    avg_profit = sum(winners) / len(winners) if winners else 0
    avg_loss = sum(losers) / len(losers) if losers else 0
    
    print(f"ì´ ê±°ë˜: {total}")
    print(f"ìŠ¹ë¥ : {win_rate:.1f}% ({len(winners)}ìŠ¹ {len(losers)}íŒ¨)")
    print(f"í‰ê·  ìˆ˜ìµ: {avg_profit:.2f}%")
    print(f"í‰ê·  ì†ì‹¤: {avg_loss:.2f}%")
    print(f"ìµœëŒ€ ìˆ˜ìµ: {max(trades):.2f}%")
    print(f"ìµœëŒ€ ì†ì‹¤: {min(trades):.2f}%")

# ì‚¬ìš©
analyze_trades_log('logs/paper/2025-01-15_trades.log')
```

**scripts/check_errors.py**:
```python
def check_error_frequency(filepath):
    """ì—ëŸ¬ ë¹ˆë„ í™•ì¸"""
    error_types = defaultdict(int)
    
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            if '[ERROR]' in line or '[CRITICAL]' in line:
                # ì—ëŸ¬ íƒ€ì… ì¶”ì¶œ
                if 'Network' in line:
                    error_types['Network'] += 1
                elif 'API' in line:
                    error_types['API'] += 1
                elif 'Database' in line:
                    error_types['Database'] += 1
                else:
                    error_types['Other'] += 1
    
    if not error_types:
        print("âœ… ì—ëŸ¬ ì—†ìŒ")
        return
    
    print("âš ï¸ ì—ëŸ¬ ë°œìƒ:")
    for error_type, count in sorted(error_types.items(), key=lambda x: -x[1]):
        print(f"  {error_type}: {count}íšŒ")

# ì‚¬ìš©
check_error_frequency('logs/paper/2025-01-15_errors.log')
```

### ì„±ëŠ¥ ë¶„ì„

**ìŠ¤í¬ë¦½íŠ¸ í™œìš©**:
```bash
# ì˜¤ëŠ˜ ìŠ¹ë¥ 
python scripts/analyze_logs.py logs/paper/$(date +%Y-%m-%d)_trades.log

# ì—ëŸ¬ ì²´í¬
python scripts/check_errors.py logs/paper/$(date +%Y-%m-%d)_errors.log

# ì£¼ê°„ í†µê³„
for file in logs/paper/*_trades.log; do
    echo "=== $file ==="
    python scripts/analyze_logs.py "$file"
done
```

---

## ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰

### ì˜ˆìƒ ìš©ëŸ‰

```
ì¼ì¼ ë¡œê·¸:
  trades.log:    ~100KB  (í•˜ë£¨ 10ê±°ë˜ ê¸°ì¤€)
  decisions.log: ~200KB  (AI í˜¸ì¶œ 30íšŒ)
  errors.log:    ~50KB   (ì •ìƒ ìš´ì˜ ì‹œ)
  debug.log:     ~5MB    (DEBUG ëª¨ë“œ)

ì´ ì¼ì¼:         ~5-6MB
ì›”ê°„:            ~150-180MB
ë…„ê°„:            ~2GB
```

### ìš©ëŸ‰ ìµœì í™”

1. **DEBUG ëª¨ë“œ ë¹„í™œì„±í™”** (í”„ë¡œë•ì…˜):
```python
# config.py
LOG_LEVEL = 'INFO'  # DEBUG ëŒ€ì‹  INFO
```

2. **ë¡œê·¸ ì••ì¶•**:
```bash
# ì˜¤ë˜ëœ ë¡œê·¸ ì••ì¶•
gzip logs/paper/*_2024-*.log
```

3. **ìë™ ì •ë¦¬**:
```bash
# crontab
# ë§¤ì›” 1ì¼ 90ì¼ ì´ìƒ ë¡œê·¸ ì‚­ì œ
0 0 1 * * find logs/ -name "*.log" -mtime +90 -delete
```

---

## .gitignore ì„¤ì •

```gitignore
# ë¡œê·¸ íŒŒì¼
logs/*.log
logs/**/*.log
logs/**/*.gz

# ë‹¨, êµ¬ì¡°ëŠ” ìœ ì§€
!logs/README.md
!logs/paper/README.md
!logs/live/README.md
!logs/backtest/README.md
```

---

## README.md (logs/)

```markdown
# Logs Directory

ì‹œìŠ¤í…œ ë¡œê·¸ ì €ì¥ì†Œ

## êµ¬ì¡°
```
logs/
â”œâ”€â”€ paper/       # ëª¨ì˜íˆ¬ì
â”œâ”€â”€ live/        # ì‹¤ê±°ë˜
â”œâ”€â”€ backtest/    # ë°±í…ŒìŠ¤íŠ¸
â””â”€â”€ reports/     # ë¦¬í¬íŠ¸
```

## ë¡œê·¸ íŒŒì¼
- `trades.log`: ê±°ë˜ ê¸°ë¡
- `decisions.log`: AI íŒë‹¨
- `errors.log`: ì—ëŸ¬ ë¡œê·¸
- `debug.log`: ì „ì²´ ë””ë²„ê·¸

## ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```bash
# ê±°ë˜ ë¡œê·¸
tail -f logs/paper/$(date +%Y-%m-%d)_trades.log

# ì—ëŸ¬ ë¡œê·¸
tail -f logs/paper/$(date +%Y-%m-%d)_errors.log
```

## ë¶„ì„
```bash
# ì˜¤ëŠ˜ í†µê³„
python scripts/analyze_logs.py logs/paper/$(date +%Y-%m-%d)_trades.log

# ì—ëŸ¬ ì²´í¬
python scripts/check_errors.py logs/paper/$(date +%Y-%m-%d)_errors.log
```

## ë³´ê´€ ê¸°ê°„
- **Paper**: 30ì¼
- **Live**: ì˜êµ¬ (ì„¸ê¸ˆ ì‹ ê³ ìš©)
- **Backtest**: 90ì¼
- **Reports**: 1ë…„
```

---

## ìë™í™” (cron/Task Scheduler)

### Linux/Mac

**crontab -e**:
```bash
# ë§¤ì¼ ìì • ë¡œê·¸ ì•„ì¹´ì´ë¸Œ
0 0 * * * cd /path/to/project && python scripts/archive_logs.py

# ë§¤ì›” 1ì¼ ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ
0 0 1 * * find /path/to/project/logs -name "*.log" -mtime +90 -delete

# ë§¤ì¼ ì˜¤ì „ 9ì‹œ ì¼ì¼ ë¦¬í¬íŠ¸
0 9 * * * cd /path/to/project && python -c "from monitoring.reporter import PerformanceReporter; r = PerformanceReporter(); print(r.generate_daily_summary())"
```

### Windows Task Scheduler

```xml
ì‘ì—… 1: ë¡œê·¸ ì•„ì¹´ì´ë¸Œ
  íŠ¸ë¦¬ê±°: ë§¤ì¼ ìì •
  ë™ì‘: python scripts/archive_logs.py

ì‘ì—… 2: ë¡œê·¸ ì •ë¦¬
  íŠ¸ë¦¬ê±°: ë§¤ì›” 1ì¼
  ë™ì‘: powershell "Get-ChildItem logs -Recurse | Where-Object {$_.LastWriteTime -lt (Get-Date).AddDays(-90)} | Remove-Item"
```

---

## ë¬¸ì œ í•´ê²°

### Q: ë¡œê·¸ íŒŒì¼ì´ ë„ˆë¬´ ì»¤ìš”
**A**: DEBUG ëª¨ë“œ ë¹„í™œì„±í™” + ë¡œê·¸ ì••ì¶•
```python
# config.py
LOG_LEVEL = 'INFO'
```

### Q: ë¡œê·¸ê°€ ê¸°ë¡ë˜ì§€ ì•Šì•„ìš”
**A**: ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸
```bash
chmod 755 logs/
```

### Q: ì‹¤ì‹œê°„ ë¡œê·¸ë¥¼ ë³´ê³  ì‹¶ì–´ìš”
**A**: tail ëª…ë ¹ ì‚¬ìš©
```bash
tail -f logs/paper/$(date +%Y-%m-%d)_trades.log
```

### Q: íŠ¹ì • ë‚ ì§œ ë¡œê·¸ë¥¼ ì°¾ê³  ì‹¶ì–´ìš”
**A**: íŒŒì¼ëª…ì´ ë‚ ì§œ ê¸°ë°˜ì´ë¯€ë¡œ ì§ì ‘ ì—´ê¸°
```bash
cat logs/paper/2025-01-10_trades.log
```

---

**ë¬¸ì„œ ë²„ì „**: v1.0  
**ì‘ì„±ì¼**: 2025-01-15  
**Phase**: 13 (ë¡œê·¸ ì‹œìŠ¤í…œ)  
**ê²€ì¦**: âœ… ì™„ë£Œ
</artifact>
